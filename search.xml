<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Welcome to maoerrrr wonderland！！</title>
    <url>/2024/01/25/welcom%20to%20maoerrrr%20wonderland/</url>
    <content><![CDATA[<p>在<font color="PINK">MAO<font color="nnnfxed">RRRR  <font color="orange">WON<font color="gold">DER<font color="#87CEFA">LA<font color="Epurple">ND</font></font></font></font></font></font>里享受吧~~~</p>
<p><font face="华文琥珀" size="4">文章推荐：<br>1.<a href="http://heisenberg-szh.com/2024/01/30/%E5%9F%BA%E4%BA%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-bert-%E7%9A%84%E7%81%BE%E9%9A%BE%E7%9C%9F%E5%AE%9E%E6%80%A7%E9%A2%84%E6%B5%8B/">基于自然语言处理(bert)的灾难信息真实性预测</a></font></p>
]]></content>
  </entry>
  <entry>
    <title>知识图谱阅读报告</title>
    <url>/2024/03/10/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/</url>
    <content><![CDATA[<h1 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h1><p>知识图谱（Knowledge Graph）起源于谷歌在2012年提出的Google Knowledge Graph，它旨在通过连接数据点来增强其搜索引擎的搜索结果的相关性和丰富性。知识图谱通常由一系列相互连接的实体和它们的属性构成，这些实体和属性通过关系相连接，形成了一个庞大的网络结构。每个实体代表现实世界中的一个对象，如人、地方、组织等，而属性则描述了实体的特性，关系则定义了实体之间的各种连接。</p>
<p>自谷歌将其商业化以来，知识图谱的概念已经在很多其他领域得到了应用，例如：</p>
<p>①搜索引擎优化：搜索引擎使用知识图谱来理解用户查询的上下文，提供更准确的搜索结果和信息摘要。</p>
<p>②推荐系统：知识图谱可以用来增强推荐系统，通过考虑用户和项目之间以及项目本身的属性和关系，提供更个性化和准确的推荐，填补稀疏，缓解冷启动问题等。</p>
<p>③医疗：知识图谱可以帮助组织和分析复杂的医学信息，从而协助疾病诊断和药物发现。其可解释性也为医疗发现提供了广阔前景。</p>
<p>·······</p>
<p>知识图谱可以为我们提供丰富的知识填充以及可解释性的大大加强，下面是两篇我近期学习和阅读知识图谱相关论文的阅读报告（含8篇paper）</p>
<h1 id="前沿技术的学习"><a href="#前沿技术的学习" class="headerlink" title="前沿技术的学习"></a>前沿技术的学习</h1><p>No.1</p>
<div class="pdfobject-container" data-target="https://cdn.jsdelivr.net/gh/Heisenberg10110/pdf@main/20240303%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A.pdf" data-height="950px"></div>


<p>No.2</p>
<div class="pdfobject-container" data-target="https://cdn.jsdelivr.net/gh/Heisenberg10110/pdf@main/20240307-10%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A.pdf" data-height="950px"></div>
]]></content>
      <categories>
        <category>paper</category>
      </categories>
      <tags>
        <tag>Knowledge Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>美赛回顾(三) · 制图</title>
    <url>/2024/02/13/%E7%BE%8E%E8%B5%9B%E5%9B%9E%E9%A1%BE-%E4%B8%89-%C2%B7%E5%88%B6%E5%9B%BE/</url>
    <content><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>在前文<font face="华文琥珀" color="blue"><a href="http://heisenberg-szh.com/2024/02/13/%E7%BE%8E%E8%B5%9B%E5%9B%9E%E9%A1%BE-%E4%BA%8C-%C2%B7%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/">美赛回顾(二) · 文本处理</a></font>中已经得到表格数据，找到了年份，月份，省份以及动物种类两两之间的关系。这篇文章将回顾将这些数据可视化的过程！<br><font face="幼圆" color="gray" size="2">以便投入到美术建模的美赛之中~~~<BR></BR></font></p>
<span id="more"></span>
<h1 id="数据形式"><a href="#数据形式" class="headerlink" title="数据形式"></a>数据形式</h1><p>这里用省份年份为例，作中国地图非法野生动物贸易数据的可视化。</p>
<table>
<thead>
<tr>
<th align="center">省份</th>
<th align="center">上海市</th>
<th align="center">云南省</th>
<th align="center">内蒙古自治区</th>
<th align="center">北京市</th>
<th align="center">台湾省</th>
</tr>
</thead>
<tbody><tr>
<td align="center">SUM</td>
<td align="center">18585</td>
<td align="center">22545</td>
<td align="center">13852</td>
<td align="center">27439</td>
<td align="center">2</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">省份</th>
<th align="center">吉林省</th>
<th align="center">四川省</th>
<th align="center">天津市</th>
<th align="center">宁夏回族自治区</th>
<th align="center">安徽省</th>
</tr>
</thead>
<tbody><tr>
<td align="center">SUM</td>
<td align="center">4260</td>
<td align="center">28498</td>
<td align="center">20377</td>
<td align="center">6097</td>
<td align="center">14374</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">省份</th>
<th align="center">山东省</th>
<th align="center">山西省</th>
<th align="center">广东省</th>
<th align="center">广西壮族自治区</th>
<th align="center">新疆维吾尔自治区</th>
</tr>
</thead>
<tbody><tr>
<td align="center">SUM</td>
<td align="center">28115</td>
<td align="center">7463</td>
<td align="center">42048</td>
<td align="center">24932</td>
<td align="center">9304</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">省份</th>
<th align="center">江苏省</th>
<th align="center">江西省</th>
<th align="center">河北省</th>
<th align="center">河南省</th>
<th align="center">浙江省</th>
</tr>
</thead>
<tbody><tr>
<td align="center">SUM</td>
<td align="center">29962</td>
<td align="center">25821</td>
<td align="center">22042</td>
<td align="center">3444</td>
<td align="center">15372</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">省份</th>
<th align="center">海南省</th>
<th align="center">湖北省</th>
<th align="center">湖南省</th>
<th align="center">澳门特别行政区</th>
<th align="center">甘肃省</th>
</tr>
</thead>
<tbody><tr>
<td align="center">SUM</td>
<td align="center">6323</td>
<td align="center">17308</td>
<td align="center">16968</td>
<td align="center">0</td>
<td align="center">3923</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">省份</th>
<th align="center">福建省</th>
<th align="center">西藏自治区</th>
<th align="center">贵州省</th>
<th align="center">辽宁省</th>
<th align="center">重庆市</th>
</tr>
</thead>
<tbody><tr>
<td align="center">SUM</td>
<td align="center">16697</td>
<td align="center">4781</td>
<td align="center">12736</td>
<td align="center">14291</td>
<td align="center">9265</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">省份</th>
<th align="center">陕西省</th>
<th align="center">青海省</th>
<th align="center">香港特别行政区</th>
<th align="center">黑龙江省</th>
</tr>
</thead>
<tbody><tr>
<td align="center">SUM</td>
<td align="center">4184</td>
<td align="center">8023</td>
<td align="center">174</td>
<td align="center">5605</td>
</tr>
</tbody></table>
<h1 id="作图"><a href="#作图" class="headerlink" title="作图"></a>作图</h1><h2 id="导入库和数据"><a href="#导入库和数据" class="headerlink" title="导入库和数据"></a>导入库和数据</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Map</span><br><span class="line"><span class="keyword">from</span> pyecharts.<span class="built_in">globals</span> <span class="keyword">import</span> ThemeType</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 获取桌面路径</span></span><br><span class="line">desktop_path = os.path.join(os.path.expanduser(<span class="string">&quot;~&quot;</span>), <span class="string">&quot;Desktop&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 CSV 文件的完整路径</span></span><br><span class="line">csv_file_path = os.path.join(desktop_path, <span class="string">&quot;sheng_year.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 pandas 读取 CSV 文件</span></span><br><span class="line">data = pd.read_csv(csv_file_path, encoding=<span class="string">&#x27;GBK&#x27;</span>)</span><br><span class="line"><span class="comment"># GBK或是UTF-8都可以试试！</span></span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="comment"># 看看数据是不是正确形式</span></span><br></pre></td></tr></table></figure>
<h2 id="创建图表"><a href="#创建图表" class="headerlink" title="创建图表"></a>创建图表</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 Map 图表</span></span><br><span class="line">map_chart = Map(init_opts=opts.InitOpts(theme=ThemeType.LIGHT))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line">province_column = data.columns[<span class="number">0</span>]</span><br><span class="line">data_column = data.columns[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 zip 结果转换为列表</span></span><br><span class="line">data_list = <span class="built_in">list</span>(<span class="built_in">zip</span>(data[province_column], data[data_column]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加数据</span></span><br><span class="line">map_chart.add(<span class="string">&quot;China Map&quot;</span>, data_list,</span><br><span class="line">              maptype=<span class="string">&quot;china&quot;</span>, label_opts=opts.LabelOpts(is_show=<span class="literal">False</span>),</span><br><span class="line">              is_map_symbol_show=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 设置全局参数</span></span><br><span class="line">map_chart.set_global_opts(</span><br><span class="line">    title_opts=opts.TitleOpts(title=<span class="string">&quot;World Map&quot;</span>),</span><br><span class="line">    visualmap_opts=opts.VisualMapOpts(</span><br><span class="line">        is_piecewise=<span class="literal">False</span>,</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义红橙黄绿蓝紫的渐变颜色</span></span><br><span class="line">        range_color=[<span class="string">&quot;#288A2B&quot;</span>, <span class="string">&quot;#EAC863&quot;</span>, <span class="string">&quot;#D84D5C&quot;</span>],</span><br><span class="line">        max_=<span class="number">42000</span>,  <span class="comment"># 最大值</span></span><br><span class="line">        min_=<span class="number">0</span>,  <span class="comment"># 最小值</span></span><br><span class="line">        orient=<span class="string">&quot;horizontal&quot;</span>,  <span class="comment"># 渐变色方向</span></span><br><span class="line">        pos_top=<span class="string">&quot;80%&quot;</span>,  <span class="comment"># 设置位置</span></span><br><span class="line">        pos_right=<span class="string">&quot;55%&quot;</span>,</span><br><span class="line">    ),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制地图</span></span><br><span class="line">map_chart.render(path=<span class="string">&quot;test_bar.html&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> webbrowser</span><br><span class="line">webbrowser.<span class="built_in">open</span>(<span class="string">&quot;test_bar.html&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><font size="5">效果：</font><br><img src="https://cdn.jsdelivr.net/gh/Heisenberg10110/images@main/img/%E5%88%B6%E5%9B%BE/china.jpg" alt="中国非法野生动物贸易数目分布"></p>
<center><i><b>Figure1</b></i><font color="gray"> | <i></i></font>中国非法野生动物贸易数目分布</center>

<h1 id="美术建模！"><a href="#美术建模！" class="headerlink" title="美术建模！"></a>美术建模！</h1><p>其他工具也很重要！！ppt和photoshop的使用可以让建模更加<font face="幼圆" color="#00a8cc" size="4">美术!</font></p>
<p>让我们来欣赏~~~~<br><img src="https://cdn.jsdelivr.net/gh/Heisenberg10110/images@main/img/%E5%88%B6%E5%9B%BE/world.jpg"></p>
<center><i><b>Figure2</b></i><font color="gray"> | <i></i></font>世界治理非法野生动物贸易兴趣（interest）分布</center>

<p><img src="https://cdn.jsdelivr.net/gh/Heisenberg10110/images@main/img/%E5%88%B6%E5%9B%BE/%E7%89%A9%E7%A7%8D.jpg"></p>
<center><i><b>Figure3</b></i><font color="gray"> | <i></i></font>中国非法野生动物贸易：物种 - 比例分布</center>

<p><img src="https://cdn.jsdelivr.net/gh/Heisenberg10110/images@main/img/%E5%88%B6%E5%9B%BE/%E7%89%A9%E7%A7%8D-%E6%9C%88%E4%BB%BD.jpg"></p>
<center><i><b>Figure4</b></i><font color="gray"> | <i></i></font>中国非法野生动物贸易：物种 - 时间分布</center>

<p><img src="https://cdn.jsdelivr.net/gh/Heisenberg10110/images@main/img/%E5%88%B6%E5%9B%BE/%E8%81%9A%E7%B1%BB.jpg"></p>
<center><i><b>Figure5</b></i><font color="gray"> | <i></i></font>世界治理非法野生动物贸易兴趣（interest）- 能力（power and resource）聚类</center>

<p><img src="https://cdn.jsdelivr.net/gh/Heisenberg10110/images@main/img/%E5%88%B6%E5%9B%BE/%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg"></p>
<center><i><b>Figure6</b></i><font color="gray"> | <i></i></font>流程图（一小时赶工现学PPT+制作o(╥﹏╥)o）</center>



<p><BR><BR></BR></BR></p>
<center><font face="华文琥珀" size="5" color="gray">纪念首次参加却收获颇多的建模大赛</font></center>]]></content>
      <categories>
        <category>美赛</category>
      </categories>
      <tags>
        <tag>figure</tag>
      </tags>
  </entry>
  <entry>
    <title>美赛回顾(二) · 文本处理</title>
    <url>/2024/02/13/%E7%BE%8E%E8%B5%9B%E5%9B%9E%E9%A1%BE-%E4%BA%8C-%C2%B7%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>由前文<font face="华文琥珀" color="blue"><a href="http://heisenberg-szh.com/2024/02/10/%E7%BE%8E%E8%B5%9B%E5%9B%9E%E9%A1%BE-%E4%B8%80-%C2%B7%E7%88%AC%E8%99%AB/">美赛回顾(一) · 爬虫</a></font>已经爬取到了若干非法野生动物贸易的审判书文本信息，现在我们尝试获取文本之中的信息并制成可用表格。</p>
<h1 id="数据形式"><a href="#数据形式" class="headerlink" title="数据形式"></a>数据形式</h1><p>- - - - - - - - - - - - - - - - - - - - -<br>Title: 云南普洱一男子非法运输穿山甲片被刑拘<br>Time: 2019-12-07 21:39:58<br>Link: <a href="https://www.cxxxxxxxt.org/article/detail/2019/12/id/4709300.shtml">https://www.cxxxxxxxt.org/article/detail/2019/12/id/4709300.shtml</a><br>Text:<br>近日，云南省普洱市宁洱县森林公安局查获一起非法运输珍贵濒危野生动物制品案，缴获国家二级保护动物穿山甲甲片0.95千克，抓获犯罪嫌疑人一名。<br>据介绍，宁洱县公安局民警在检查站开展公开查缉时，发现一名男子所骑的摩托车后备箱内一个塑料袋有疑似野生动物穿山甲制品，于是把案件移交给宁洱县森林公安局。<br>经审讯，摩托车驾驶员吕某昌交代，一个朋友托他从境外买点穿山甲片做药引子，并通过微信向他转了2000多元人民币。吕某昌随后在境外某集市花500多元人民币购买了这些穿山甲片，并驾驶自己的摩托车非法将穿山甲片运输入境。<br>经鉴定，这些制品确系国家二级保护野生动物穿山甲的甲片，数量为0.95千克。因涉嫌非法收购、运输珍贵濒危野生动物制品，吕某昌目前已被公安机关刑事拘留，案件还在进一步办理中。<br>- - - - - - - - - - - - - - - - - - - - -</p>
<span id="more"></span>
<h1 id="文本分析"><a href="#文本分析" class="headerlink" title="文本分析"></a>文本分析</h1><p>由于美赛时间紧迫，需要较短时间得出可用数据，私以为提取手法是过于粗糙的。今适逢春节，后续大创也事情繁多，这里就不重新修改，源码也就不放出来了T.T<br>简单说说大概思路吧~</p>
<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>把多次爬取的文本直接合成为一个word录入字符串变量，建立set集合，把链接相同的文章去除，达到不重复的目的。</p>
<h2 id="提取文本信息"><a href="#提取文本信息" class="headerlink" title="提取文本信息"></a>提取文本信息</h2><p>设立滑窗，提取关键文本（如龟，竹鼠，虎 ···）附近的语段，采用正则表达式或自然语言模型如word2vec，甚至用本体提取等知识图谱相关知识可以提取地点，年份，人物，野生动物种类，数目等信息（如0.95千克穿山甲；316只竹鼠····）</p>
<h1 id="有趣的点"><a href="#有趣的点" class="headerlink" title="有趣的点"></a>有趣的点</h1><p>采取字典可以便利地录入信息：<br>每一个link作为key便于提取文章的信息。</p>
<p>采用多层字典，便捷地生成多层信息！</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 年份字典</span></span><br><span class="line">year_phrases = [<span class="number">2016</span>, <span class="number">2017</span>, <span class="number">2018</span>, <span class="number">2019</span>, <span class="number">2020</span>, <span class="number">2021</span>, <span class="number">2022</span>, <span class="number">2023</span>, <span class="number">2024</span>]</span><br><span class="line">year_phrases_dict = &#123;year: defaultdict(<span class="built_in">int</span>) <span class="keyword">for</span> year <span class="keyword">in</span> year_phrases&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 省份内层字典</span></span><br><span class="line">sheng_phrases = [ </span><br><span class="line"><span class="string">&quot;河北&quot;</span>, <span class="string">&quot;山西&quot;</span>, <span class="string">&quot;辽宁&quot;</span>, <span class="string">&quot;吉林&quot;</span>, <span class="string">&quot;黑龙江&quot;</span>, <span class="string">&quot;江苏&quot;</span>, <span class="string">&quot;浙江&quot;</span>, <span class="string">&quot;安徽&quot;</span>, </span><br><span class="line"><span class="string">&quot;福建&quot;</span>, <span class="string">&quot;江西&quot;</span>, <span class="string">&quot;山东&quot;</span>, <span class="string">&quot;河南&quot;</span>, <span class="string">&quot;湖北&quot;</span>, <span class="string">&quot;湖南&quot;</span>, <span class="string">&quot;广东&quot;</span>, <span class="string">&quot;海南&quot;</span>, </span><br><span class="line"><span class="string">&quot;四川&quot;</span>, <span class="string">&quot;贵州&quot;</span>, <span class="string">&quot;云南&quot;</span>, <span class="string">&quot;陕西&quot;</span>,<span class="string">&quot;甘肃&quot;</span>, <span class="string">&quot;青海&quot;</span>, <span class="string">&quot;台湾&quot;</span>, <span class="string">&quot;内蒙&quot;</span>, </span><br><span class="line"><span class="string">&quot;广西&quot;</span>, <span class="string">&quot;西藏&quot;</span>, <span class="string">&quot;宁夏&quot;</span>, <span class="string">&quot;新疆&quot;</span>, <span class="string">&quot;北京&quot;</span>, <span class="string">&quot;天津&quot;</span>, <span class="string">&quot;上海&quot;</span>, <span class="string">&quot;重庆&quot;</span>, <span class="string">&quot;香港&quot;</span>, <span class="string">&quot;澳门&quot;</span>]</span><br><span class="line">·</span><br><span class="line">·</span><br><span class="line">·</span><br><span class="line">·</span><br><span class="line">·</span><br><span class="line">·</span><br><span class="line"><span class="keyword">for</span> year_ <span class="keyword">in</span> year_phrases_dict:</span><br><span class="line">  <span class="keyword">if</span> year_ == year:</span><br><span class="line">      year_phrases_dict[year][sheng] += extract_and_sum_numbers(sentence)</span><br></pre></td></tr></table></figure>
<BR>

<p>建立字典时定义了内层省份字典是defaultdict，遇到新出现的key时自动初始化为0，达到二维化数据的目的！稍后把提取到的数据变为Excel等文件就变得十分容易！</p>
</BR>]]></content>
      <categories>
        <category>美赛</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>美赛回顾(一) · 爬虫</title>
    <url>/2024/02/10/%E7%BE%8E%E8%B5%9B%E5%9B%9E%E9%A1%BE-%E4%B8%80-%C2%B7%E7%88%AC%E8%99%AB/</url>
    <content><![CDATA[<h2 id="SPIDER！"><a href="#SPIDER！" class="headerlink" title="SPIDER！"></a>SPIDER！</h2><p>前不久才刚刚初步学习了爬虫相关知识，跟随教程简单爬取了医院和招聘网站的信息，没想到在美赛实现了如此规模（<em><strong>170万字</strong></em>）的爬虫！！<font color="gold" size="5">Spider haunts at night！！ </font></p>
<span id="more"></span>
<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p><font size="4"><center><strong>Problem F: Reducing Illegal Wildlife Trade</strong></center></font></p>
<!-- ![保护野生动物！](https://github.com/Heisenberg10110/images/blob/main/img/hu.png?raw=true) -->

<p><img src="https://cdn.jsdelivr.net/gh/Heisenberg10110/images@a3bff6235056131ac745bec7edf13129a95b0315/img/%E7%88%AC%E8%99%AB/hu.png" alt="保护野生动物！"><br></p>
<blockquote>
<p><font face="斜体"><strong>Illegal wildlife trade</strong> negatively impacts our environment and threatens global biodiversity. It is<br>estimated to involve up to 26.5 billion US dollars per year and is considered to be the fourth<br>largest of all global illegal trades.<br>[1] You are to develop a data-driven 5-year project designed to make a notable reduction in illegal wildlife trade. Your goal is to convince a client to carry out your project. To do this, you must select both a client and an appropriate project for that client.<br>Your work should explore the following sub-questions:<br>● Who is your client? What can that client realistically do? (In other words, your client should have the powers, resources, and interest needed to enact the project you propose.)<br>● Explain why the project you developed is suitable for this client. What research, from published literature and from your own analyses, supports the selection of your proposed project? Using a data-driven analysis, how will you convince your client that this is a project they should undertake?<br>● What additional powers and resources will your client need to carry out the project? (Remember to use assumptions, but also ground your work in reality as much as you are able.)<br>● If the project is carried out what will happen? In other words, what will the measurable impact on illegal wildlife trade be? What analysis did you do to determine this?<br>● How likely is the project to reach the expected goal? Also, based on a contextualized sensitivity analysis, are there conditions or events that may disproportionately aid or harm the project’s ability to reach its goal?<br>While you could limit your approach to illegal wildlife trade, you may also consider illegal wildlife trade as part of a larger complex system. Specifically, you could consider how other global efforts in other domains, e.g., efforts to curtail other forms of trafficking or efforts to reduce climate change coupled with efforts to curtail illegal wildlife trade, may be part of a complex system. This may create synergistic opportunities for unexpected actors in this domain.<br>If you choose to leverage a complexity framework in your solution, be sure to justify your choice by discussing the benefits and drawbacks of this modeling decision.Additionally, your team must submit a 1-page memo with key points for your client, highlighting your 5-year project proposal and why the project is right for them as a client (e.g., access to resources, part of their mandate, aligns with their mission statement, etc.) </font></p>
</blockquote>
<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>经过一番国际层面的研究，我们把最终的客户锁定在中国。于是，找到中国非法贸易的数据并分析成为了目标，可以从非法贸易的审判书等资料获取。</p>
<h2 id="Spider！"><a href="#Spider！" class="headerlink" title="Spider！"></a>Spider！</h2><p>导入所需的库</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<h3 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">driver = webdriver.Edge()</span><br><span class="line">base_url = <span class="string">&#x27;https://www.cxxxxxxxt.org&#x27;</span>  </span><br><span class="line"><span class="comment"># 找到网站</span></span><br><span class="line">search_url = <span class="string">&#x27;/article/essearch/keyword/%E6%BF%92%E5%8D%B1%E9%87%8E%E7%94%9F%E5%8A%A8%E7%89%A9/ticket/tr035hZedznczbY7Ag34pg1wQntELscZW2dBZKhpuNgKwxD1WX7e4Kig_5yZvIrabqf3ozOiJGrYsZT8-ZeJtjykVi42K8IMCfTc5eJKE0UgmzoUqX-zb4oO4A%2A%2A/appid/2056206409/randstr/%40WT4/content_time_publish_begin/2016-01-01/content_time_publish_end/2024-02-03/page/&#x27;</span></span><br><span class="line"><span class="comment"># 定位————根据页数 为后续翻页形成模板</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">page_url = <span class="string">f&#x27;<span class="subst">&#123;base_url&#125;</span><span class="subst">&#123;search_url&#125;</span><span class="subst">&#123;<span class="number">1</span>&#125;</span>.shtml&#x27;</span></span><br><span class="line"><span class="comment"># 定位第一页</span></span><br><span class="line"></span><br><span class="line">driver.get(page_url)</span><br><span class="line">html_content = driver.page_source.encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">soup = BeautifulSoup(html_content, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">pagination_div = soup.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;paginationControl&#x27;</span>)</span><br><span class="line"><span class="comment"># 发送HTTP请求并获取页面内容</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">last_page_link = soup.find(<span class="string">&#x27;a&#x27;</span>, text=<span class="string">&#x27;尾页&#x27;</span>)</span><br><span class="line"><span class="comment"># 定位尾页</span></span><br><span class="line"><span class="keyword">if</span> last_page_link:</span><br><span class="line">    last_page_url = last_page_link[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line">    <span class="comment"># 提取尾页的页码信息</span></span><br><span class="line">    max_page = <span class="built_in">int</span>(last_page_url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>].split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;总共有 <span class="subst">&#123;max_page&#125;</span> 页&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;未找到页码信息&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>到这里，我们可以实现对搜索内容相关的每一页，每一篇文章进行遍历</p>
<h3 id="爬取"><a href="#爬取" class="headerlink" title="爬取"></a>爬取</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> docx <span class="keyword">import</span> Document</span><br><span class="line">doc = Document() </span><br><span class="line"><span class="comment"># 打开我们要存入的word文档</span></span><br><span class="line"></span><br><span class="line">std = <span class="number">0</span></span><br><span class="line"><span class="comment">#为遍历文章数计数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> page_number <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, max_page+<span class="number">1</span>):  <span class="comment"># 从首页遍历到尾页</span></span><br><span class="line">    page_url = <span class="string">f&#x27;<span class="subst">&#123;base_url&#125;</span><span class="subst">&#123;search_url&#125;</span><span class="subst">&#123;page_number&#125;</span>.shtml&#x27;</span></span><br><span class="line">    driver.get(page_url)</span><br><span class="line">    <span class="comment">#定位某一页</span></span><br><span class="line"></span><br><span class="line">    html_content = driver.page_source.encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    soup = BeautifulSoup(html_content, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查找&lt;dt&gt;标签下的所有链接</span></span><br><span class="line">    dt_tags = soup.find_all(<span class="string">&#x27;dt&#x27;</span>)</span><br><span class="line">    <span class="comment"># 好好观察F12页面各个参数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历每个链接</span></span><br><span class="line">    <span class="keyword">for</span> dt_tag <span class="keyword">in</span> dt_tags:</span><br><span class="line">        std = std + <span class="number">1</span></span><br><span class="line">        a_tag = dt_tag.find(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> a_tag:</span><br><span class="line">            relative_link = a_tag.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将相对链接转换为绝对链接，找到文章的链接</span></span><br><span class="line">            absolute_link = urljoin(base_url, relative_link)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 访问链接并获取页面内容</span></span><br><span class="line">            driver.get(absolute_link)</span><br><span class="line">            html_content1 = driver.page_source.encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">            soup1 = BeautifulSoup(html_content1, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 提取标题</span></span><br><span class="line">            title = soup1.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;detail_bigtitle&#x27;</span>)</span><br><span class="line">            title_text = title.get_text(strip=<span class="literal">True</span>) <span class="keyword">if</span> title <span class="keyword">else</span> <span class="string">&quot;No title found&quot;</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                time_span = soup1.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;detail_thr&#x27;</span>).find(<span class="string">&#x27;span&#x27;</span>, class_=<span class="string">&#x27;time&#x27;</span>)</span><br><span class="line">                <span class="comment"># 有的文章已经过期，提取time会失败，采用try来规避</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;no time now&quot;</span>)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 提取时间</span></span><br><span class="line">            timeo = time_span.get_text(strip=<span class="literal">True</span>) <span class="comment"># 与time库重，不能取time变量名</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 提取主体文字</span></span><br><span class="line">            detail_txt_div = soup1.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;detail_txt&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> detail_txt_div: <span class="comment"># if也是一种规避方法</span></span><br><span class="line">                paragraphs = detail_txt_div.find_all(<span class="string">&#x27;p&#x27;</span>) <span class="comment"># 每一段用&#x27;p&#x27;隔开</span></span><br><span class="line">                link_text = <span class="string">&#x27;\n&#x27;</span>.join([p.get_text(strip=<span class="literal">True</span>) <span class="keyword">for</span> p <span class="keyword">in</span> paragraphs])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 写入打开的word文档</span></span><br><span class="line">                doc.add_paragraph(<span class="string">f&quot;Title: <span class="subst">&#123;title_text&#125;</span>&quot;</span>)</span><br><span class="line">                doc.add_paragraph(<span class="string">f&quot;Time: <span class="subst">&#123;timeo&#125;</span>&quot;</span>)</span><br><span class="line">                doc.add_paragraph(<span class="string">f&quot;Link: <span class="subst">&#123;absolute_link&#125;</span>&quot;</span>)</span><br><span class="line">                doc.add_paragraph(<span class="string">f&quot;Text:\n<span class="subst">&#123;link_text&#125;</span>&quot;</span>)</span><br><span class="line">                doc.add_paragraph(<span class="string">&#x27;-&#x27;</span> * <span class="number">30</span>)</span><br><span class="line">                random_sleep_time = random.uniform(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">                <span class="comment"># 执行随机停顿</span></span><br><span class="line">                time.sleep(random_sleep_time)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;已经爬了<span class="subst">&#123;std&#125;</span>篇文章，<span class="subst">&#123;page_number&#125;</span>页，还剩<span class="subst">&#123;max_page-page_number&#125;</span>页&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存Word文档</span></span><br><span class="line">doc.save(<span class="string">&#x27;output.docx&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭浏览器</span></span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>
<h2 id="完工！"><a href="#完工！" class="headerlink" title="完工！"></a>完工！</h2><p><img src="https://cdn.jsdelivr.net/gh/Heisenberg10110/images@a3bff6235056131ac745bec7edf13129a95b0315/img/%E7%88%AC%E8%99%AB/%E5%AD%97%E6%95%B0.png"></p>
<p>170万字 1400+篇文章！！！</p>
]]></content>
      <categories>
        <category>美赛</category>
      </categories>
      <tags>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title>基于自然语言处理(bert)的灾难信息真实性预测</title>
    <url>/2024/01/30/%E5%9F%BA%E4%BA%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-bert-%E7%9A%84%E7%81%BE%E9%9A%BE%E7%9C%9F%E5%AE%9E%E6%80%A7%E9%A2%84%E6%B5%8B/</url>
    <content><![CDATA[<h1 id="基于自然语言处理-bert-的灾难信息真实性预测"><a href="#基于自然语言处理-bert-的灾难信息真实性预测" class="headerlink" title="基于自然语言处理(bert)的灾难信息真实性预测"></a>基于自然语言处理(bert)的灾难信息真实性预测</h1><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>&emsp;&emsp;微信朋友圈，微博，Twitter上有着众多用户的分享。可以成为紧急情况下的重要沟通渠道。智能手机的无处不在使人们能够实时宣布他们正在观察到的紧急情况。正因为如此，越来越多的机构（即救灾组织和新闻机构）对以程序化方式监控 Twitter 感兴趣。<br>&emsp;&emsp;但是，并不总是很清楚一个人的话是否真的在宣布一场灾难，正因如此，我们想要搭建一个模型，分析文段的信息，从而判断灾难信息的真实性。</p>
<span id="more"></span>
<h2 id="数据形式"><a href="#数据形式" class="headerlink" title="数据形式"></a>数据形式</h2><p>如：<br><font size="4"><center>训练集</center></font></p>
<table>
<thead>
<tr>
<th align="center">id</th>
<th align="center">关键词</th>
<th align="center">位置</th>
<th align="center">文本</th>
<th align="center">真实性</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">ablaze</td>
<td align="center">Bangkok</td>
<td align="center">On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">disaster</td>
<td align="center">school</td>
<td align="center">homework！！！o(╥﹏╥)o</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">10900</td>
<td align="center">accident</td>
<td align="center">Gloucestershire , UK</td>
<td align="center">.@NorwayMFA #Bahrain police had previously died in a road accident they were not killed by explosion…</td>
<td align="center">1</td>
</tr>
</tbody></table>
<h2 id="关键"><a href="#关键" class="headerlink" title="关键"></a>关键</h2><p>&emsp;&emsp;对关键词，位置以及本次问题的关键————文本进行编码嵌入，涉及到自然语言处理问题（NLP）。</p>
<h3 id="①常规编码嵌入"><a href="#①常规编码嵌入" class="headerlink" title="①常规编码嵌入"></a>①常规编码嵌入</h3><p>&emsp;&emsp;关键词（keyword）以及位置仅由少量的词（大多为一个），采用onehot编码，是对于一般字符类型数据的常规embedding方式。</p>
<h3 id="②NLP嵌入"><a href="#②NLP嵌入" class="headerlink" title="②NLP嵌入"></a>②NLP嵌入</h3><h4 id="词袋法："><a href="#词袋法：" class="headerlink" title="词袋法："></a>词袋法：</h4><p>&emsp;&emsp;类似于onehot嵌入，把文本中每个词都提取出来（可以省略is a the等词）形成词袋（set集合）然后类似于onehot进行编码。</p>
<h4 id="bert嵌入："><a href="#bert嵌入：" class="headerlink" title="bert嵌入："></a>bert嵌入：</h4><blockquote>
<p>BERT 是 Bidirectional Encoder Representations from Transformers 的首字母缩写词。</p>
<p>早在 2018 年，谷歌就为 NLP 应用程序开发了一个基于 Transformer 的强大的机器学习模型，该模型在不同的基准数据集中优于以前的语言模型。这个模型被称为BERT。</p>
<p>BERT 架构由多个堆叠在一起的 Transformer 编码器组成。每个 Transformer 编码器都封装了两个子层：一个自注意力层和一个前馈层。</p>
</blockquote>
<p>基于attention的bert在深度学习的技术下使得文本的特征提取能力大大提高。</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>导入库和数据：<br></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line">train = pd.read_csv(<span class="string">&#x27;/kaggle/input/nlpgettingstarted/train.csv&#x27;</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;/kaggle/input/nlpgettingstarted/test.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">id</span> = test[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">test.drop([<span class="string">&#x27;id&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">train.drop([<span class="string">&#x27;id&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 由于id值对于训练集来说不重要，直接去除，检验集为后续对应预测保留id信息，而模型中需要特征的对应，也删去id一列</span></span><br><span class="line">target = train[<span class="string">&#x27;target&#x27;</span>]</span><br><span class="line">train.drop([<span class="string">&#x27;target&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 灾难信息真实性的信息作为label</span></span><br></pre></td></tr></table></figure>
<p>对关键词，地理位置的空缺值进行简单的填充值’0’,进行onehot编码</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">train = train.sample(frac=<span class="number">1</span>, random_state=<span class="number">42</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># 进行随机排序</span></span><br><span class="line"></span><br><span class="line">train.columns.values[<span class="number">4</span>] = <span class="string">&#x27;targeta&#x27;</span></span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;keyword&#x27;</span>].fillna(<span class="string">&#x27;0&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test[<span class="string">&#x27;keyword&#x27;</span>].fillna(<span class="string">&#x27;0&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;location&#x27;</span>].fillna(<span class="string">&#x27;0&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test[<span class="string">&#x27;location&#x27;</span>].fillna(<span class="string">&#x27;0&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">encoder = OneHotEncoder(sparse_output=<span class="literal">False</span>, drop=<span class="string">&#x27;first&#x27;</span>,handle_unknown=<span class="string">&#x27;ignore&#x27;</span>)  <span class="comment"># drop=&#x27;first&#x27; 表示去除第一类别（防止过拟合）</span></span><br><span class="line"></span><br><span class="line">encoded = encoder.fit_transform(train[[<span class="string">&#x27;keyword&#x27;</span>, <span class="string">&#x27;location&#x27;</span>]])</span><br><span class="line">train = pd.concat([train.drop([<span class="string">&#x27;keyword&#x27;</span>, <span class="string">&#x27;location&#x27;</span>], axis=<span class="number">1</span>), pd.DataFrame(encoded, columns=encoder.get_feature_names_out( [<span class="string">&#x27;keyword&#x27;</span>, <span class="string">&#x27;location&#x27;</span>]))], axis=<span class="number">1</span>) <span class="comment">#拼接</span></span><br><span class="line"></span><br><span class="line">encoded = encoder.transform(test[[<span class="string">&#x27;keyword&#x27;</span>, <span class="string">&#x27;location&#x27;</span>]])</span><br><span class="line">test = pd.concat([test.drop([<span class="string">&#x27;keyword&#x27;</span>, <span class="string">&#x27;location&#x27;</span>], axis=<span class="number">1</span>), pd.DataFrame(encoded, columns=encoder.get_feature_names_out( [<span class="string">&#x27;keyword&#x27;</span>, <span class="string">&#x27;location&#x27;</span>]))], axis=<span class="number">1</span>) <span class="comment">#拼接</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="自然语言处理（NLP）"><a href="#自然语言处理（NLP）" class="headerlink" title="自然语言处理（NLP）"></a>自然语言处理（NLP）</h2><p>这里给出两种方法：</p>
<h3 id="使用bert进行编码："><a href="#使用bert进行编码：" class="headerlink" title="使用bert进行编码："></a>使用bert进行编码：</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 GPU 是否可用</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练的BERT模型和分词器</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">model.to(device)  <span class="comment"># 将BERT模型移动到GPU上</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode_text</span>(<span class="params">text</span>):</span><br><span class="line">    tokens = tokenizer(text, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line">    tokens.to(device)  <span class="comment"># 将输入张量移动到GPU上</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        output = model(**tokens)</span><br><span class="line">    <span class="keyword">return</span> output[<span class="string">&#x27;last_hidden_state&#x27;</span>].mean(dim=<span class="number">1</span>).squeeze().cpu().numpy()  <span class="comment"># 将输出张量移动回CPU</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>tokenizer: 这是一个分词器对象，它执行文本到模型输入格式之间的转换。这通常是一个预训练语言模型（例如BERT、GPT等）的tokenizer。</p>
<p>text: 这是要被分词和编码的原始文本。</p>
<p>padding&#x3D;True: 这个参数指示分词器是否要在序列的末尾添加填充标记，以使所有输入序列具有相同的长度。填充对于批量处理是很有用的，因为它允许将不同长度的序列组合成一个张量。</p>
<p>truncation&#x3D;True: 这个参数表示如果文本长度超过模型的最大输入长度，是否要截断文本。截断可确保所有输入都具有相同的长度。</p>
<p>return_tensors&#x3D;’pt’: 这个参数指示tokenizer返回PyTorch张量。这是因为PyTorch是一个深度学习框架，如果你正在使用PyTorch构建和训练模型，你可能希望输入数据以PyTorch张量的形式提供。</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">train[<span class="string">&#x27;text_encoded&#x27;</span>] = train[<span class="string">&#x27;text&#x27;</span>].apply(encode_text)</span><br><span class="line">test[<span class="string">&#x27;text_encoded&#x27;</span>] = test[<span class="string">&#x27;text&#x27;</span>].apply(encode_text)</span><br><span class="line"><span class="comment"># 进行bert编码，提取出众多特征列，可以直接作为常规数据形式喂给模型</span></span><br><span class="line"></span><br><span class="line">train.drop(<span class="string">&#x27;text&#x27;</span>, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>) </span><br><span class="line">test.drop(<span class="string">&#x27;text&#x27;</span>, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 删去原文本列</span></span><br></pre></td></tr></table></figure>
<h4 id="展开bert编码生成的text-encoded向量列"><a href="#展开bert编码生成的text-encoded向量列" class="headerlink" title="展开bert编码生成的text_encoded向量列"></a>展开bert编码生成的text_encoded向量列</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">text_encoded_expanded = pd.DataFrame(train[<span class="string">&#x27;text_encoded&#x27;</span>].to_list(), columns=[<span class="string">f&#x27;text_encoded_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(train[<span class="string">&#x27;text_encoded&#x27;</span>].iloc[<span class="number">0</span>]))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将拆分后的列与原始 DataFrame 合并</span></span><br><span class="line">train = pd.concat([train, text_encoded_expanded], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 删除原始 &#x27;text_encoded&#x27; 列</span></span><br><span class="line">train.drop([<span class="string">&#x27;text_encoded&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">text_encoded_expanded = pd.DataFrame(test[<span class="string">&#x27;text_encoded&#x27;</span>].to_list(), columns=[<span class="string">f&#x27;text_encoded_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test[<span class="string">&#x27;text_encoded&#x27;</span>].iloc[<span class="number">0</span>]))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将拆分后的列与原始 DataFrame 合并</span></span><br><span class="line">test = pd.concat([test, text_encoded_expanded], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 删除原始 &#x27;text_encoded&#x27; 列</span></span><br><span class="line">test.drop([<span class="string">&#x27;text_encoded&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><font color="blue">final数据形式：</font><br></p>
<table>
<thead>
<tr>
<th align="center">keyword_ablaze</th>
<th align="center">keyword_accident</th>
<th align="center">keyword_aftershock</th>
<th align="center">···</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.0</td>
<td align="center">1.0</td>
<td align="center">0.0</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">···</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">location_Bangkok</th>
<th align="center">location_school</th>
<th align="center">location_plane</th>
<th align="center">···</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">0.0</td>
<td align="center">1.0</td>
<td align="center">0.0</td>
<td align="center">···</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">text_encoded_0</th>
<th align="center">text_encoded_1</th>
<th align="center">text_encoded_767</th>
<th align="center">···</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.262901</td>
<td align="center">-0.57036</td>
<td align="center">0.66548</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">-0.862901</td>
<td align="center">0.37036</td>
<td align="center">-0.06548</td>
<td align="center">···</td>
</tr>
</tbody></table>
<h3 id="使用词袋编码（损失了词义和语义，单纯的词的统计形式编码）："><a href="#使用词袋编码（损失了词义和语义，单纯的词的统计形式编码）：" class="headerlink" title="使用词袋编码（损失了词义和语义，单纯的词的统计形式编码）："></a>使用词袋编码（损失了词义和语义，单纯的词的统计形式编码）：</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理文本列</span></span><br><span class="line">text_vectorizer = CountVectorizer()</span><br><span class="line">train_text_vectors = text_vectorizer.fit_transform(train[<span class="string">&#x27;text&#x27;</span>])</span><br><span class="line">test_text_vectors = text_vectorizer.transform(test[<span class="string">&#x27;text&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(train.head())</span><br><span class="line"><span class="comment"># 将得到的文本特征表示添加到原始 DataFrame 中</span></span><br><span class="line">train_text_df = pd.DataFrame(train_text_vectors.toarray(), columns=text_vectorizer.get_feature_names_out())</span><br><span class="line">test_text_df = pd.DataFrame(test_text_vectors.toarray(), columns=text_vectorizer.get_feature_names_out())</span><br><span class="line"><span class="built_in">print</span>(train.head())</span><br><span class="line">train = pd.concat([train, train_text_df], axis=<span class="number">1</span>)</span><br><span class="line">test = pd.concat([test, test_text_df], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">train.drop(<span class="string">&#x27;text&#x27;</span>, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test.drop(<span class="string">&#x27;text&#x27;</span>, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 删去原文本列</span></span><br></pre></td></tr></table></figure>
<h2 id="喂给模型前处理"><a href="#喂给模型前处理" class="headerlink" title="喂给模型前处理"></a>喂给模型前处理</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(train, target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment"># 训练集，验证集的划分</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()<span class="comment"># 创建 StandardScaler</span></span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_val = scaler.transform(X_val)</span><br><span class="line">test = scaler.transform(test)</span><br><span class="line"><span class="comment"># 标准化（转化为均值为0，方差为1的数据）利用模型对特征的提取</span></span><br></pre></td></tr></table></figure>
<h2 id="深度学习模型（DEEP-LEARNING）"><a href="#深度学习模型（DEEP-LEARNING）" class="headerlink" title="深度学习模型（DEEP LEARNING）"></a>深度学习模型（DEEP LEARNING）</h2><p>这里只采用简单的全连接<br><font size="2" color="gray" face="幼圆">(也只试过全连接T.T)</font></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, TensorDataset</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    torch.cuda.manual_seed(<span class="number">2</span>)</span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line">X_train_tensor = torch.FloatTensor(X_train.values)</span><br><span class="line">y_train_tensor = torch.FloatTensor(y_train.values)</span><br><span class="line">X_val_tensor = torch.FloatTensor(X_val.values)</span><br><span class="line">y_val_tensor = torch.FloatTensor(y_val.values)</span><br><span class="line"><span class="comment"># 转化数据为torch的tensor张量形式</span></span><br><span class="line">X_train_tensor = X_train_tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">y_train_tensor = y_train_tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">X_val_tensor = X_val_tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">y_val_tensor = y_val_tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="comment"># 把数据变量放置到cuda（GPU）上运行</span></span><br></pre></td></tr></table></figure>
<p>准备开始啦！ 搭建全连接框架</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, <span class="number">1024</span>)</span><br><span class="line">        self.fc1024 = nn.Linear(<span class="number">1024</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">1024</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">256</span>, <span class="number">64</span>)</span><br><span class="line">        self.fc5 = nn.Linear(<span class="number">64</span>,<span class="number">1</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line">        <span class="comment"># 线性层以及最终二分类目的的sigmoid函数（多分类用softmax）</span></span><br><span class="line"></span><br><span class="line">        self.bn1024 = nn.BatchNorm1d(<span class="number">1024</span>)</span><br><span class="line">        self.bn512 = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.bn256 = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        self.bn128 = nn.BatchNorm1d(<span class="number">128</span>)</span><br><span class="line">        self.bn64 = nn.BatchNorm1d(<span class="number">64</span>)</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>BN<br>通常指的是批归一化（Batch Normalization）</p>
<ol>
<li>提高模型训练的稳定性和加速收敛。</li>
<li>减小对初始权重的依赖，允许使用更高的学习率。</li>
<li>有轻微的正则化效果，有助于减小过拟合。</li>
</ol>
</blockquote>
<blockquote>
<p>Dropout <br>目的是在训练过程中随机地丢弃（关闭）神经网络的一些单元（节点），以减小网络的复杂性，防止过拟合。</p>
<ol>
<li>减小过拟合风险，提高模型的泛化能力。</li>
<li>降低对某些特定神经元的过度依赖，使模型更加健壮。</li>
</ol>
</blockquote>
<blockquote>
<p>ReLU（Rectified Linear Unit）<br>是一种常用的激活函数，广泛用于深度学习模型中。ReLU 将所有负数输入变为零，而对于正数输入则保持不变。ReLU 函数的定义如下：<br>f(x)&#x3D;max(0,x)</p>
<ol>
<li>更加有效率的梯度下降以及反向传播：避免了梯度爆炸和梯度消失问题。</li>
<li>简化计算过程：没有了其他复杂激活函数中诸如指数函数的影响；同时活跃度的分散性使得神经网络整体计算成本下降。</li>
<li>其重要的非线性性质帮助我们在理论上可以拟合任意函数。</li>
</ol>
</blockquote>
<p>前向传播</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    x = self.fc1(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.bn1024(x)</span><br><span class="line">    x = self.dropout(x)</span><br><span class="line">    </span><br><span class="line">    x = self.fc1024(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.bn1024(x)</span><br><span class="line">    x = self.dropout(x)</span><br><span class="line">    </span><br><span class="line">    x = self.fc2(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.bn512(x)</span><br><span class="line">    x = self.dropout(x)</span><br><span class="line">    </span><br><span class="line">    x = self.fc3(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.bn256(x)</span><br><span class="line">    x = self.dropout(x)</span><br><span class="line">    </span><br><span class="line">    x = self.fc4(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.bn64(x)</span><br><span class="line">    </span><br><span class="line">    x = self.fc5(x)</span><br><span class="line">    </span><br><span class="line">    x = self.sigmoid(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">    <span class="comment"># dim变化：model-1024-1024-512-256-64-1-sigmoid</span></span><br></pre></td></tr></table></figure>
<p>模型创建和参数设置（炼丹o(╥﹏╥)o）</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = SimpleNN(input_size=X_train_tensor.shape[<span class="number">1</span>])</span><br><span class="line">model = model.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> LambdaLR, StepLR</span><br><span class="line"><span class="comment">#warmip器 学习率先上升“摸索地图” 后按指数函数衰减</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lr_lambda</span>(<span class="params">epoch, learning_rate, increase_epochs</span>):</span><br><span class="line">    <span class="keyword">if</span> epoch &lt; increase_epochs:</span><br><span class="line">        <span class="keyword">return</span> (epoch + <span class="number">1</span>) / increase_epochs</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span>  <span class="number">0.95</span> ** (epoch - increase_epochs)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">50</span></span><br><span class="line">increase_epochs = <span class="number">5</span></span><br><span class="line">learning_rate = <span class="number">0.0005</span></span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scheduler = LambdaLR(optimizer, lr_lambda=<span class="keyword">lambda</span> epoch:lr_lambda(epoch, learning_rate, increase_epochs))</span><br><span class="line"><span class="comment"># 将数据转换为 DataLoader</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">3350</span></span><br><span class="line">train_dataset = TensorDataset(X_train_tensor, y_train_tensor)</span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">val_dataset = TensorDataset(X_val_tensor, y_val_tensor)</span><br><span class="line">val_loader = DataLoader(val_dataset, batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">best_val_loss = <span class="number">100</span> <span class="comment"># 为找到最小损失的模型，其实用最高acc可能更好？！</span></span><br></pre></td></tr></table></figure>
<p>开始学习！</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    train_loss = <span class="number">0.0</span> <span class="comment"># 勿忘初始化</span></span><br><span class="line">    train_preds = []</span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 梯度归零</span></span><br><span class="line">        outputs = model(inputs.to(<span class="string">&#x27;cuda&#x27;</span>)).squeeze()</span><br><span class="line">        loss = criterion(outputs, labels.to(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">        loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment"># 更新模型参数</span></span><br><span class="line">        train_loss += loss.item()  <span class="comment"># 累积每个批次的损失</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算整个训练集上的预测结果和准确率</span></span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式，防止影响 BatchNormalization 和 Dropout 等层的行为</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        all_train_preds = []</span><br><span class="line">        <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">            outputs = model(inputs.to(<span class="string">&#x27;cuda&#x27;</span>)).squeeze()</span><br><span class="line">            preds = (outputs &gt; <span class="number">0.5</span>).<span class="built_in">int</span>().cpu().numpy()  <span class="comment"># 将预测转换为numpy数组</span></span><br><span class="line">            all_train_preds.append(preds)</span><br><span class="line"></span><br><span class="line">    all_train_preds = np.concatenate(all_train_preds)</span><br><span class="line">    <span class="comment"># 计算整个训练集上的准确率</span></span><br><span class="line">    train_accuracy = accuracy_score(y_train_tensor.cpu().numpy(), all_train_preds)</span><br><span class="line">    train_loss /= <span class="built_in">len</span>(train_loader)  <span class="comment"># 平均每个批次的损失</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在验证集上评估模型</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        all_val_preds = []</span><br><span class="line">        val_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> val_loader:</span><br><span class="line">            outputs = model(inputs.to(<span class="string">&#x27;cuda&#x27;</span>)).squeeze()</span><br><span class="line">            batch_loss = criterion(outputs, labels.to(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">            val_loss += batch_loss.item()  <span class="comment"># 累积每个批次的损失</span></span><br><span class="line">            preds = (outputs &gt; <span class="number">0.5</span>).<span class="built_in">int</span>()  <span class="comment"># 使用阈值 0.5 进行二分类</span></span><br><span class="line">            all_val_preds.append(preds.cpu().numpy())</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">        all_val_preds = np.concatenate(all_val_preds)</span><br><span class="line">        val_loss /= <span class="built_in">len</span>(val_loader)  <span class="comment"># 计算平均验证集损失</span></span><br><span class="line">        val_accuracy = accuracy_score(y_val_tensor.cpu().numpy(), all_val_preds)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>]\n Train Loss: <span class="subst">&#123;train_loss:<span class="number">.6</span>f&#125;</span>,Val Loss: <span class="subst">&#123;val_loss:<span class="number">.6</span>f&#125;</span>\nTrain Accuracy: <span class="subst">&#123;train_accuracy:<span class="number">.6</span>f&#125;</span>, Val Accuracy: <span class="subst">&#123;val_accuracy:<span class="number">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Learning rate: <span class="subst">&#123;optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]&#125;</span>\n\n&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">            best_val_loss = val_loss</span><br><span class="line">            best_model_params = model.state_dict()  <span class="comment"># 保存最佳模型参数</span></span><br><span class="line">        </span><br><span class="line">        scheduler.step()   <span class="comment"># 调整学习率</span></span><br></pre></td></tr></table></figure>
<p>训练过程：</p>
<blockquote>
<p>True<br>Epoch [1&#x2F;50]Train Loss: 0.726889,Val Loss: 0.695472<br>Train Accuracy: 0.427915, Val Accuracy: 0.436638<br>Learning rate: 0.0001</p>
<p>Epoch [2&#x2F;50]<br>Train Loss: 0.722656,Val Loss: 0.692767<br>Train Accuracy: 0.542857, Val Accuracy: 0.556139<br>Learning rate: 0.0002</p>
<p>Epoch [3&#x2F;50]<br>Train Loss: 0.715571,Val Loss: 0.690732<br>Train Accuracy: 0.572085, Val Accuracy: 0.563362<br>Learning rate: 0.0003</p>
<p>·   ·   ·<br>·   ·   ·<br>·   ·   ·<br>·   ·   ·</p>
<p>Epoch [50&#x2F;50]<br>Train Loss: 0.178646,Val Loss: 0.588852<br>Train Accuracy: 0.949589, Val Accuracy: 0.818122<br>Learning rate: 5.2336e-05</p>
</blockquote>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>得到了模型，开始预测吧！</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用模型进行预测</span></span><br><span class="line">test_tensor = torch.FloatTensor(test.values).to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型加载为最佳模型的参数</span></span><br><span class="line">model.load_state_dict(best_model_params)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    predictions = model(test_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 GPU 上的张量移动到 CPU</span></span><br><span class="line">predictions = predictions.cpu().numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择其中一列数据，例如选择第一列（索引 0）</span></span><br><span class="line">target_predictions = predictions[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将概率值超过 0.5 的标记为 1，低于 0.5 的标记为 0</span></span><br><span class="line">binary_predictions = (target_predictions &gt; <span class="number">0.5</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">binary_predictions = binary_predictions.ravel()</span><br><span class="line"><span class="built_in">print</span>(binary_predictions)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;\n\n<span class="subst">&#123;test.columns=&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建包含 &#x27;id&#x27; 和 &#x27;targeta&#x27; 列的 DataFrame</span></span><br><span class="line">submission_df = pd.DataFrame(&#123;<span class="string">&#x27;id&#x27;</span>: <span class="built_in">id</span>, <span class="string">&#x27;target&#x27;</span>: binary_predictions&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据保存到 CSV 文件</span></span><br><span class="line">submission_df.to_csv(<span class="string">&#x27;prebert.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>第一次进行自然语言处理。attention is all you need！！google的bert真是太棒了。最终达到了0.80386分（324&#x2F;894）~~~<BR>不过参数调整还有优化空间！炼丹！！！( * ^▽^ * )</BR></p>
<h2 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h2><p>loss最优改为acc最优啦，然后调节batch到3700，模型准确率最终在0.827！ 303&#x2F;894了！</p>
]]></content>
      <categories>
        <category>kaggle</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
</search>
