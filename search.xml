<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Welcome to maoerrrr wonderland！！</title>
    <url>/2024/01/25/welcom%20to%20maoerrrr%20wonderland/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>基于自然语言处理(bert)的灾难信息真实性预测</title>
    <url>/2024/01/30/%E5%9F%BA%E4%BA%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86-bert-%E7%9A%84%E7%81%BE%E9%9A%BE%E7%9C%9F%E5%AE%9E%E6%80%A7%E9%A2%84%E6%B5%8B/</url>
    <content><![CDATA[<h1 id="基于自然语言处理-bert-的灾难信息真实性预测"><a href="#基于自然语言处理-bert-的灾难信息真实性预测" class="headerlink" title="基于自然语言处理(bert)的灾难信息真实性预测"></a>基于自然语言处理(bert)的灾难信息真实性预测</h1><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>&emsp;&emsp;微信朋友圈，微博，Twitter上有着众多用户的分享。可以成为紧急情况下的重要沟通渠道。智能手机的无处不在使人们能够实时宣布他们正在观察到的紧急情况。正因为如此，越来越多的机构（即救灾组织和新闻机构）对以程序化方式监控 Twitter 感兴趣。<br>&emsp;&emsp;但是，并不总是很清楚一个人的话是否真的在宣布一场灾难，正因如此，我们想要搭建一个模型，分析文段的信息，从而判断灾难信息的真实性。</p>
<h2 id="数据形式"><a href="#数据形式" class="headerlink" title="数据形式"></a>数据形式</h2><p>如：<br><font size="4"><center>训练集</center></font></p>
<table>
<thead>
<tr>
<th align="center">id</th>
<th align="center">关键词</th>
<th align="center">位置</th>
<th align="center">文本</th>
<th align="center">真实性</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">ablaze</td>
<td align="center">Bangkok</td>
<td align="center">On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">disaster</td>
<td align="center">school</td>
<td align="center">homework！！！o(╥﹏╥)o</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">10900</td>
<td align="center">accident</td>
<td align="center">Gloucestershire , UK</td>
<td align="center">.@NorwayMFA #Bahrain police had previously died in a road accident they were not killed by explosion…</td>
<td align="center">1</td>
</tr>
</tbody></table>
<h2 id="关键"><a href="#关键" class="headerlink" title="关键"></a>关键</h2><p>&emsp;&emsp;对关键词，位置以及本次问题的关键————文本进行编码嵌入，涉及到自然语言处理问题（NLP）。</p>
<h3 id="①常规编码嵌入"><a href="#①常规编码嵌入" class="headerlink" title="①常规编码嵌入"></a>①常规编码嵌入</h3><p>&emsp;&emsp;关键词（keyword）以及位置仅由少量的词（大多为一个），采用onehot编码，是对于一般字符类型数据的常规embedding方式。</p>
<h3 id="②NLP嵌入"><a href="#②NLP嵌入" class="headerlink" title="②NLP嵌入"></a>②NLP嵌入</h3><h4 id="词袋法："><a href="#词袋法：" class="headerlink" title="词袋法："></a>词袋法：</h4><p>&emsp;&emsp;类似于onehot嵌入，把文本中每个词都提取出来（可以省略is a the等词）形成词袋（set集合）然后类似于onehot进行编码。</p>
<h4 id="bert嵌入："><a href="#bert嵌入：" class="headerlink" title="bert嵌入："></a>bert嵌入：</h4><blockquote>
<p>BERT 是 Bidirectional Encoder Representations from Transformers 的首字母缩写词。</p>
<p>早在 2018 年，谷歌就为 NLP 应用程序开发了一个基于 Transformer 的强大的机器学习模型，该模型在不同的基准数据集中优于以前的语言模型。这个模型被称为BERT。</p>
<p>BERT 架构由多个堆叠在一起的 Transformer 编码器组成。每个 Transformer 编码器都封装了两个子层：一个自注意力层和一个前馈层。</p>
</blockquote>
<p>基于attention的bert在深度学习的技术下使得文本的特征提取能力大大提高。</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>导入库和数据：<br></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line">train = pd.read_csv(<span class="string">&#x27;/kaggle/input/nlpgettingstarted/train.csv&#x27;</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">&#x27;/kaggle/input/nlpgettingstarted/test.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">id</span> = test[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">test.drop([<span class="string">&#x27;id&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">train.drop([<span class="string">&#x27;id&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 由于id值对于训练集来说不重要，直接去除，检验集为后续对应预测保留id信息，而模型中需要特征的对应，也删去id一列</span></span><br><span class="line">target = train[<span class="string">&#x27;target&#x27;</span>]</span><br><span class="line">train.drop([<span class="string">&#x27;target&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 灾难信息真实性的信息作为label</span></span><br></pre></td></tr></table></figure>
<p>对关键词，地理位置的空缺值进行简单的填充值’0’,进行onehot编码</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">train = train.sample(frac=<span class="number">1</span>, random_state=<span class="number">42</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># 进行随机排序</span></span><br><span class="line"></span><br><span class="line">train.columns.values[<span class="number">4</span>] = <span class="string">&#x27;targeta&#x27;</span></span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;keyword&#x27;</span>].fillna(<span class="string">&#x27;0&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test[<span class="string">&#x27;keyword&#x27;</span>].fillna(<span class="string">&#x27;0&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train[<span class="string">&#x27;location&#x27;</span>].fillna(<span class="string">&#x27;0&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test[<span class="string">&#x27;location&#x27;</span>].fillna(<span class="string">&#x27;0&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">encoder = OneHotEncoder(sparse_output=<span class="literal">False</span>, drop=<span class="string">&#x27;first&#x27;</span>,handle_unknown=<span class="string">&#x27;ignore&#x27;</span>)  <span class="comment"># drop=&#x27;first&#x27; 表示去除第一类别（防止过拟合）</span></span><br><span class="line"></span><br><span class="line">encoded = encoder.fit_transform(train[[<span class="string">&#x27;keyword&#x27;</span>, <span class="string">&#x27;location&#x27;</span>]])</span><br><span class="line">train = pd.concat([train.drop([<span class="string">&#x27;keyword&#x27;</span>, <span class="string">&#x27;location&#x27;</span>], axis=<span class="number">1</span>), pd.DataFrame(encoded, columns=encoder.get_feature_names_out( [<span class="string">&#x27;keyword&#x27;</span>, <span class="string">&#x27;location&#x27;</span>]))], axis=<span class="number">1</span>) <span class="comment">#拼接</span></span><br><span class="line"></span><br><span class="line">encoded = encoder.transform(test[[<span class="string">&#x27;keyword&#x27;</span>, <span class="string">&#x27;location&#x27;</span>]])</span><br><span class="line">test = pd.concat([test.drop([<span class="string">&#x27;keyword&#x27;</span>, <span class="string">&#x27;location&#x27;</span>], axis=<span class="number">1</span>), pd.DataFrame(encoded, columns=encoder.get_feature_names_out( [<span class="string">&#x27;keyword&#x27;</span>, <span class="string">&#x27;location&#x27;</span>]))], axis=<span class="number">1</span>) <span class="comment">#拼接</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="自然语言处理（NLP）"><a href="#自然语言处理（NLP）" class="headerlink" title="自然语言处理（NLP）"></a>自然语言处理（NLP）</h2><p>这里给出两种方法：</p>
<h3 id="使用bert进行编码："><a href="#使用bert进行编码：" class="headerlink" title="使用bert进行编码："></a>使用bert进行编码：</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 GPU 是否可用</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练的BERT模型和分词器</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line">model.to(device)  <span class="comment"># 将BERT模型移动到GPU上</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode_text</span>(<span class="params">text</span>):</span><br><span class="line">    tokens = tokenizer(text, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line">    tokens.to(device)  <span class="comment"># 将输入张量移动到GPU上</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        output = model(**tokens)</span><br><span class="line">    <span class="keyword">return</span> output[<span class="string">&#x27;last_hidden_state&#x27;</span>].mean(dim=<span class="number">1</span>).squeeze().cpu().numpy()  <span class="comment"># 将输出张量移动回CPU</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>tokenizer: 这是一个分词器对象，它执行文本到模型输入格式之间的转换。这通常是一个预训练语言模型（例如BERT、GPT等）的tokenizer。</p>
<p>text: 这是要被分词和编码的原始文本。</p>
<p>padding&#x3D;True: 这个参数指示分词器是否要在序列的末尾添加填充标记，以使所有输入序列具有相同的长度。填充对于批量处理是很有用的，因为它允许将不同长度的序列组合成一个张量。</p>
<p>truncation&#x3D;True: 这个参数表示如果文本长度超过模型的最大输入长度，是否要截断文本。截断可确保所有输入都具有相同的长度。</p>
<p>return_tensors&#x3D;’pt’: 这个参数指示tokenizer返回PyTorch张量。这是因为PyTorch是一个深度学习框架，如果你正在使用PyTorch构建和训练模型，你可能希望输入数据以PyTorch张量的形式提供。</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">train[<span class="string">&#x27;text_encoded&#x27;</span>] = train[<span class="string">&#x27;text&#x27;</span>].apply(encode_text)</span><br><span class="line">test[<span class="string">&#x27;text_encoded&#x27;</span>] = test[<span class="string">&#x27;text&#x27;</span>].apply(encode_text)</span><br><span class="line"><span class="comment"># 进行bert编码，提取出众多特征列，可以直接作为常规数据形式喂给模型</span></span><br><span class="line"></span><br><span class="line">train.drop(<span class="string">&#x27;text&#x27;</span>, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>) </span><br><span class="line">test.drop(<span class="string">&#x27;text&#x27;</span>, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 删去原文本列</span></span><br></pre></td></tr></table></figure>
<h4 id="展开bert编码生成的text-encoded向量列"><a href="#展开bert编码生成的text-encoded向量列" class="headerlink" title="展开bert编码生成的text_encoded向量列"></a>展开bert编码生成的text_encoded向量列</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">text_encoded_expanded = pd.DataFrame(train[<span class="string">&#x27;text_encoded&#x27;</span>].to_list(), columns=[<span class="string">f&#x27;text_encoded_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(train[<span class="string">&#x27;text_encoded&#x27;</span>].iloc[<span class="number">0</span>]))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将拆分后的列与原始 DataFrame 合并</span></span><br><span class="line">train = pd.concat([train, text_encoded_expanded], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 删除原始 &#x27;text_encoded&#x27; 列</span></span><br><span class="line">train.drop([<span class="string">&#x27;text_encoded&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">text_encoded_expanded = pd.DataFrame(test[<span class="string">&#x27;text_encoded&#x27;</span>].to_list(), columns=[<span class="string">f&#x27;text_encoded_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test[<span class="string">&#x27;text_encoded&#x27;</span>].iloc[<span class="number">0</span>]))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将拆分后的列与原始 DataFrame 合并</span></span><br><span class="line">test = pd.concat([test, text_encoded_expanded], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 删除原始 &#x27;text_encoded&#x27; 列</span></span><br><span class="line">test.drop([<span class="string">&#x27;text_encoded&#x27;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><font color="gold">final数据形式：</font><br></p>
<table>
<thead>
<tr>
<th align="center">keyword_ablaze</th>
<th align="center">keyword_accident</th>
<th align="center">keyword_aftershock</th>
<th align="center">···</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.0</td>
<td align="center">1.0</td>
<td align="center">0.0</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">···</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">location_Bangkok</th>
<th align="center">location_school</th>
<th align="center">location_plane</th>
<th align="center">···</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">0.0</td>
<td align="center">1.0</td>
<td align="center">0.0</td>
<td align="center">···</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">text_encoded_0</th>
<th align="center">text_encoded_1</th>
<th align="center">text_encoded_767</th>
<th align="center">···</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.262901</td>
<td align="center">-0.57036</td>
<td align="center">0.66548</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
<td align="center">···</td>
</tr>
<tr>
<td align="center">-0.862901</td>
<td align="center">0.37036</td>
<td align="center">-0.06548</td>
<td align="center">···</td>
</tr>
</tbody></table>
<h3 id="使用词袋编码（损失了词义和语义，单纯的词的统计形式编码）："><a href="#使用词袋编码（损失了词义和语义，单纯的词的统计形式编码）：" class="headerlink" title="使用词袋编码（损失了词义和语义，单纯的词的统计形式编码）："></a>使用词袋编码（损失了词义和语义，单纯的词的统计形式编码）：</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理文本列</span></span><br><span class="line">text_vectorizer = CountVectorizer()</span><br><span class="line">train_text_vectors = text_vectorizer.fit_transform(train[<span class="string">&#x27;text&#x27;</span>])</span><br><span class="line">test_text_vectors = text_vectorizer.transform(test[<span class="string">&#x27;text&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(train.head())</span><br><span class="line"><span class="comment"># 将得到的文本特征表示添加到原始 DataFrame 中</span></span><br><span class="line">train_text_df = pd.DataFrame(train_text_vectors.toarray(), columns=text_vectorizer.get_feature_names_out())</span><br><span class="line">test_text_df = pd.DataFrame(test_text_vectors.toarray(), columns=text_vectorizer.get_feature_names_out())</span><br><span class="line"><span class="built_in">print</span>(train.head())</span><br><span class="line">train = pd.concat([train, train_text_df], axis=<span class="number">1</span>)</span><br><span class="line">test = pd.concat([test, test_text_df], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">train.drop(<span class="string">&#x27;text&#x27;</span>, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test.drop(<span class="string">&#x27;text&#x27;</span>, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 删去原文本列</span></span><br></pre></td></tr></table></figure>
<h2 id="喂给模型前处理"><a href="#喂给模型前处理" class="headerlink" title="喂给模型前处理"></a>喂给模型前处理</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(train, target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment"># 训练集，验证集的划分</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()<span class="comment"># 创建 StandardScaler</span></span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_val = scaler.transform(X_val)</span><br><span class="line">test = scaler.transform(test)</span><br><span class="line"><span class="comment"># 标准化（转化为均值为0，方差为1的数据）利用模型对特征的提取</span></span><br></pre></td></tr></table></figure>
<h2 id="深度学习模型（DEEP-LEARNING）"><a href="#深度学习模型（DEEP-LEARNING）" class="headerlink" title="深度学习模型（DEEP LEARNING）"></a>深度学习模型（DEEP LEARNING）</h2><p>这里只采用简单的全连接<br><font size="2" color="gray" face="幼圆">(也只试过全连接T.T)</font></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, TensorDataset</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    torch.cuda.manual_seed(<span class="number">2</span>)</span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br><span class="line">X_train_tensor = torch.FloatTensor(X_train.values)</span><br><span class="line">y_train_tensor = torch.FloatTensor(y_train.values)</span><br><span class="line">X_val_tensor = torch.FloatTensor(X_val.values)</span><br><span class="line">y_val_tensor = torch.FloatTensor(y_val.values)</span><br><span class="line"><span class="comment"># 转化数据为torch的tensor张量形式</span></span><br><span class="line">X_train_tensor = X_train_tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">y_train_tensor = y_train_tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">X_val_tensor = X_val_tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">y_val_tensor = y_val_tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="comment"># 把数据变量放置到cuda（GPU）上运行</span></span><br></pre></td></tr></table></figure>
<p>准备开始啦！ 搭建全连接框架</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNN, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, <span class="number">1024</span>)</span><br><span class="line">        self.fc1024 = nn.Linear(<span class="number">1024</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">1024</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">256</span>, <span class="number">64</span>)</span><br><span class="line">        self.fc5 = nn.Linear(<span class="number">64</span>,<span class="number">1</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line">        <span class="comment"># 线性层以及最终二分类目的的sigmoid函数（多分类用softmax）</span></span><br><span class="line"></span><br><span class="line">        self.bn1024 = nn.BatchNorm1d(<span class="number">1024</span>)</span><br><span class="line">        self.bn512 = nn.BatchNorm1d(<span class="number">512</span>)</span><br><span class="line">        self.bn256 = nn.BatchNorm1d(<span class="number">256</span>)</span><br><span class="line">        self.bn128 = nn.BatchNorm1d(<span class="number">128</span>)</span><br><span class="line">        self.bn64 = nn.BatchNorm1d(<span class="number">64</span>)</span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>BN<br>通常指的是批归一化（Batch Normalization）</p>
<ol>
<li>提高模型训练的稳定性和加速收敛。</li>
<li>减小对初始权重的依赖，允许使用更高的学习率。</li>
<li>有轻微的正则化效果，有助于减小过拟合。</li>
</ol>
</blockquote>
<blockquote>
<p>Dropout <br>目的是在训练过程中随机地丢弃（关闭）神经网络的一些单元（节点），以减小网络的复杂性，防止过拟合。</p>
<ol>
<li>减小过拟合风险，提高模型的泛化能力。</li>
<li>降低对某些特定神经元的过度依赖，使模型更加健壮。</li>
</ol>
</blockquote>
<blockquote>
<p>ReLU（Rectified Linear Unit）<br>是一种常用的激活函数，广泛用于深度学习模型中。ReLU 将所有负数输入变为零，而对于正数输入则保持不变。ReLU 函数的定义如下：<br>f(x)&#x3D;max(0,x)</p>
<ol>
<li>更加有效率的梯度下降以及反向传播：避免了梯度爆炸和梯度消失问题。</li>
<li>简化计算过程：没有了其他复杂激活函数中诸如指数函数的影响；同时活跃度的分散性使得神经网络整体计算成本下降。</li>
<li>其重要的非线性性质帮助我们在理论上可以拟合任意函数。</li>
</ol>
</blockquote>
<p>前向传播</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    x = self.fc1(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.bn1024(x)</span><br><span class="line">    x = self.dropout(x)</span><br><span class="line">    </span><br><span class="line">    x = self.fc1024(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.bn1024(x)</span><br><span class="line">    x = self.dropout(x)</span><br><span class="line">    </span><br><span class="line">    x = self.fc2(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.bn512(x)</span><br><span class="line">    x = self.dropout(x)</span><br><span class="line">    </span><br><span class="line">    x = self.fc3(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.bn256(x)</span><br><span class="line">    x = self.dropout(x)</span><br><span class="line">    </span><br><span class="line">    x = self.fc4(x)</span><br><span class="line">    x = self.relu(x)</span><br><span class="line">    x = self.bn64(x)</span><br><span class="line">    </span><br><span class="line">    x = self.fc5(x)</span><br><span class="line">    </span><br><span class="line">    x = self.sigmoid(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">    <span class="comment"># dim变化：model-1024-1024-512-256-64-1-sigmoid</span></span><br></pre></td></tr></table></figure>
<p>模型创建和参数设置（炼丹o(╥﹏╥)o）</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = SimpleNN(input_size=X_train_tensor.shape[<span class="number">1</span>])</span><br><span class="line">model = model.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> LambdaLR, StepLR</span><br><span class="line"><span class="comment">#warmip器 学习率先上升“摸索地图” 后按指数函数衰减</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lr_lambda</span>(<span class="params">epoch, learning_rate, increase_epochs</span>):</span><br><span class="line">    <span class="keyword">if</span> epoch &lt; increase_epochs:</span><br><span class="line">        <span class="keyword">return</span> (epoch + <span class="number">1</span>) / increase_epochs</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span>  <span class="number">0.95</span> ** (epoch - increase_epochs)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">50</span></span><br><span class="line">increase_epochs = <span class="number">5</span></span><br><span class="line">learning_rate = <span class="number">0.0005</span></span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">scheduler = LambdaLR(optimizer, lr_lambda=<span class="keyword">lambda</span> epoch:lr_lambda(epoch, learning_rate, increase_epochs))</span><br><span class="line"><span class="comment"># 将数据转换为 DataLoader</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">3350</span></span><br><span class="line">train_dataset = TensorDataset(X_train_tensor, y_train_tensor)</span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">val_dataset = TensorDataset(X_val_tensor, y_val_tensor)</span><br><span class="line">val_loader = DataLoader(val_dataset, batch_size, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">best_val_loss = <span class="number">100</span> <span class="comment"># 为找到最小损失的模型，其实用最高acc可能更好？！</span></span><br></pre></td></tr></table></figure>
<p>开始学习！</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    train_loss = <span class="number">0.0</span> <span class="comment"># 勿忘初始化</span></span><br><span class="line">    train_preds = []</span><br><span class="line">    <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 梯度归零</span></span><br><span class="line">        outputs = model(inputs.to(<span class="string">&#x27;cuda&#x27;</span>)).squeeze()</span><br><span class="line">        loss = criterion(outputs, labels.to(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">        loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment"># 更新模型参数</span></span><br><span class="line">        train_loss += loss.item()  <span class="comment"># 累积每个批次的损失</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算整个训练集上的预测结果和准确率</span></span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式，防止影响 BatchNormalization 和 Dropout 等层的行为</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        all_train_preds = []</span><br><span class="line">        <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">            outputs = model(inputs.to(<span class="string">&#x27;cuda&#x27;</span>)).squeeze()</span><br><span class="line">            preds = (outputs &gt; <span class="number">0.5</span>).<span class="built_in">int</span>().cpu().numpy()  <span class="comment"># 将预测转换为numpy数组</span></span><br><span class="line">            all_train_preds.append(preds)</span><br><span class="line"></span><br><span class="line">    all_train_preds = np.concatenate(all_train_preds)</span><br><span class="line">    <span class="comment"># 计算整个训练集上的准确率</span></span><br><span class="line">    train_accuracy = accuracy_score(y_train_tensor.cpu().numpy(), all_train_preds)</span><br><span class="line">    train_loss /= <span class="built_in">len</span>(train_loader)  <span class="comment"># 平均每个批次的损失</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在验证集上评估模型</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        all_val_preds = []</span><br><span class="line">        val_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> val_loader:</span><br><span class="line">            outputs = model(inputs.to(<span class="string">&#x27;cuda&#x27;</span>)).squeeze()</span><br><span class="line">            batch_loss = criterion(outputs, labels.to(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">            val_loss += batch_loss.item()  <span class="comment"># 累积每个批次的损失</span></span><br><span class="line">            preds = (outputs &gt; <span class="number">0.5</span>).<span class="built_in">int</span>()  <span class="comment"># 使用阈值 0.5 进行二分类</span></span><br><span class="line">            all_val_preds.append(preds.cpu().numpy())</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">        all_val_preds = np.concatenate(all_val_preds)</span><br><span class="line">        val_loss /= <span class="built_in">len</span>(val_loader)  <span class="comment"># 计算平均验证集损失</span></span><br><span class="line">        val_accuracy = accuracy_score(y_val_tensor.cpu().numpy(), all_val_preds)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>]\n Train Loss: <span class="subst">&#123;train_loss:<span class="number">.6</span>f&#125;</span>,Val Loss: <span class="subst">&#123;val_loss:<span class="number">.6</span>f&#125;</span>\nTrain Accuracy: <span class="subst">&#123;train_accuracy:<span class="number">.6</span>f&#125;</span>, Val Accuracy: <span class="subst">&#123;val_accuracy:<span class="number">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Learning rate: <span class="subst">&#123;optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]&#125;</span>\n\n&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> val_loss &lt; best_val_loss:</span><br><span class="line">            best_val_loss = val_loss</span><br><span class="line">            best_model_params = model.state_dict()  <span class="comment"># 保存最佳模型参数</span></span><br><span class="line">        </span><br><span class="line">        scheduler.step()   <span class="comment"># 调整学习率</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>True<br>Epoch [1&#x2F;50]<br>Train Loss: 0.726889,Val Loss: 0.695472<BR><br>Train Accuracy: 0.427915, Val Accuracy: 0.&gt;436638<br><BR>Learning rate: 0.0001</BR></BR></p>
<p>Epoch [2&#x2F;50]<br>Train Loss: 0.722656,Val Loss: 0.692767<BR><br>Train Accuracy: 0.542857, Val Accuracy: 0.556139<br><BR> Learning rate: 0.0002</BR></BR></p>
<p>Epoch [3&#x2F;50]<br>Train Loss: 0.715571,Val Loss: 0.690732<BR><br>Train Accuracy: 0.572085, Val Accuracy: 0.563362<br><BR>Learning rate: 0.0003<BR>·   ·   ·<BR>·   ·   ·<BR>·   ·   ·<BR>·   ·   ·<br><BR>Epoch [50&#x2F;50]<br>Train Loss: 0.178646,Val Loss: 0.588852<BR><br>Train Accuracy: 0.949589, Val Accuracy: 0.818122<br><BR>Learning rate: 5.2336e-05</BR></BR></BR></BR></BR></BR></BR></BR></BR></p>
</blockquote>
<p>得到了模型，开始预测吧！</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用模型进行预测</span></span><br><span class="line">test_tensor = torch.FloatTensor(test.values).to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型加载为最佳模型的参数</span></span><br><span class="line">model.load_state_dict(best_model_params)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    predictions = model(test_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 GPU 上的张量移动到 CPU</span></span><br><span class="line">predictions = predictions.cpu().numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择其中一列数据，例如选择第一列（索引 0）</span></span><br><span class="line">target_predictions = predictions[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将概率值超过 0.5 的标记为 1，低于 0.5 的标记为 0</span></span><br><span class="line">binary_predictions = (target_predictions &gt; <span class="number">0.5</span>).astype(<span class="built_in">int</span>)</span><br><span class="line">binary_predictions = binary_predictions.ravel()</span><br><span class="line"><span class="built_in">print</span>(binary_predictions)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;\n\n<span class="subst">&#123;test.columns=&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建包含 &#x27;id&#x27; 和 &#x27;targeta&#x27; 列的 DataFrame</span></span><br><span class="line">submission_df = pd.DataFrame(&#123;<span class="string">&#x27;id&#x27;</span>: <span class="built_in">id</span>, <span class="string">&#x27;target&#x27;</span>: binary_predictions&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据保存到 CSV 文件</span></span><br><span class="line">submission_df.to_csv(<span class="string">&#x27;prebert.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>第一次进行自然语言处理。attention is all you need！！google的bert真是太棒了。最终达到了0.80386分（324&#x2F;894）~~~<BR>不过参数调整还有优化空间！炼丹！！！( * ^▽^ * )</BR></p>
]]></content>
      <categories>
        <category>kaggle</category>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
</search>
